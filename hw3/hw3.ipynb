{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Intoduction to Data Science\n",
    "\n",
    "## Due Tuesday, June 5, start of class\n",
    "\n",
    "### Jake Pitkin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Scrape Github Repository List and Repository Information using BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2 Download the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the first 10 pages of the highly starred git repos\n",
    "for i in range(1, 11):\n",
    "    url = \"https://github.com/search?o=desc&p=\" + str(i) + \"&q=stars%3A%3E1&s=stars&type=Repositories\"\n",
    "    # here we actually access the website\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        html = response.read()\n",
    "        html = html.decode('utf-8')\n",
    "\n",
    "    # save the file\n",
    "    with open(\"git_pages/page_\" + str(i) + \".html\", 'w') as new_file:\n",
    "        new_file.write(html)\n",
    "    time.sleep(10)\n",
    "\n",
    "# Read 10 pages from disk and create soup objects\n",
    "pages = []\n",
    "for i in range(1, 11):\n",
    "    page = BeautifulSoup(open(\"git_pages/page_\" + str(i) + \".html\"), \"html.parser\")\n",
    "    pages.append(page)\n",
    "    \n",
    "# Extract the repository links\n",
    "repo_links = []\n",
    "for i in range(10):\n",
    "    for repo_link in pages[i].select(\".v-align-middle\"):\n",
    "        try:\n",
    "            repo_links.append(\"https://github.com/\" + repo_link['href'])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Download each repository page\n",
    "for i, repo_link in enumerate(repo_links):\n",
    "    # here we actually access the website\n",
    "    with urllib.request.urlopen(repo_link) as response:\n",
    "        html = response.read()\n",
    "        html = html.decode('utf-8')\n",
    "\n",
    "    # save the file\n",
    "    with open(\"repo_pages/repo_\" + str(i) + \".html\", 'w') as new_file:\n",
    "        new_file.write(html)\n",
    "    time.sleep(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Extract Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the repository links from saved html files\n",
    "pages = []\n",
    "for i in range(1, 11):\n",
    "    page = BeautifulSoup(open(\"git_pages/page_\" + str(i) + \".html\"), \"html.parser\")\n",
    "    pages.append(page)\n",
    "repo_links = []\n",
    "for i in range(10):\n",
    "    for repo_link in pages[i].select(\".v-align-middle\"):\n",
    "        try:\n",
    "            repo_links.append(\"https://github.com/\" + repo_link['href'])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Read repo pages from disk and create soup objects\n",
    "repo_pages = []\n",
    "for i in range(100):\n",
    "    page = BeautifulSoup(open(\"repo_pages/repo_\" + str(i) + \".html\"), \"html.parser\")\n",
    "    repo_pages.append(page)\n",
    "\n",
    "# Create an entry for each repository\n",
    "project_info = pd.DataFrame(columns=['name', 'url', 'commits', 'forks', 'contributors', 'stars', 'issues', 'len_readme'])\n",
    "\n",
    "for index, repo_page in enumerate(repo_pages):\n",
    "    try:\n",
    "        url = repo_links[index]\n",
    "        name = url.split('/')[-1]\n",
    "        commits = repo_page.select(\".commits\")[0].select(\".num\")[0].get_text().replace(\" \", \"\").replace(\"\\n\", \"\")\n",
    "        forks = repo_page.select(\".social-count\")[2].get_text().replace(\" \", \"\").replace(\"\\n\", \"\")\n",
    "        contributors = repo_page.select(\".numbers-summary\")[0].select(\".num\")[3].get_text().replace(\" \", \"\").replace(\"\\n\", \"\")\n",
    "        stars = repo_page.select(\".social-count\")[1].get_text().replace(\" \", \"\").replace(\"\\n\", \"\")\n",
    "        issues = repo_page.select(\".Counter\")[0].get_text().replace(\" \", \"\").replace(\"\\n\", \"\")\n",
    "        len_readme = len(repo_page.select(\"#readme\")[0].get_text())\n",
    "    except:\n",
    "        print(\"fail\")\n",
    "        continue\n",
    "    project_info = project_info.append({'name':name, 'url':url, 'commits':commits, 'forks':forks,\n",
    "                                        'contributors':contributors, 'stars':stars, 'issues':issues, \n",
    "                                        'len_readme':len_readme}, ignore_index=True)\n",
    "\n",
    "# Save dataframe to disk\n",
    "project_info.to_csv(\"project_info.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Use linear regression to analyze the Github repository data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in the csv\n",
    "project_info = pd.read_csv(\"project_info.csv\", sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
