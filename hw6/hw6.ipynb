{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 & 2: Data acquisition and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(paths):\n",
    "    \"\"\" Extracts features and labels from a dataframe.\n",
    "\n",
    "    Args: \n",
    "        path: A list of paths to a collection of .csv's in the following dataset:\n",
    "            https://archive.ics.uci.edu/ml/datasets/Character+Font+Images\n",
    "\n",
    "    Returns:\n",
    "        X: A #samples x 20 x 20 numpy array containing the pixel data\n",
    "           for the font samples. The pixel values are originally grayscale\n",
    "           [0, 255] but scaled to the range [0, 1].\n",
    "\n",
    "        y: A numpy array containing the ascii code for each of the sample images.\n",
    "    \"\"\"\n",
    "    cols = [\"r\"+str(x)+\"c\"+str(y) for x in range(20) for y in range(20)]\n",
    "    dfs = []\n",
    "    for path in paths:\n",
    "        df = pd.read_csv(path)\n",
    "        dfs.append(df)\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    features = df[cols].copy()\n",
    "    scale = np.vectorize(lambda x: x / 255)\n",
    "    X = scale(np.reshape(features.values, features.shape[0] * features.shape[1]))\n",
    "    X = np.reshape(X, (-1, 20, 20, 1))\n",
    "    y = df['m_label'].copy().as_matrix()\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def convert_to_one_hot(y, unique_chars):\n",
    "    \"\"\" Returns a nSamples x nUniqueCharacters array which is a \n",
    "        one-hot representation of y.\n",
    "\n",
    "    Args:\n",
    "        y: A 1D array containing categorical labels.\n",
    "        \n",
    "        unique_chars: a list of the possible values in y.\n",
    "\n",
    "    Returns:\n",
    "        one_hot_rep: a one-hot representation of y.\n",
    "    \"\"\"\n",
    "    one_hot_rep = []\n",
    "    char_to_ix = { ch:i for i,ch in enumerate(unique_chars) }\n",
    "    for label in y:\n",
    "        one_hot = [0 for x in range(len(unique_chars))]\n",
    "        one_hot[char_to_ix[label]] = 1\n",
    "        one_hot_rep.append(one_hot)\n",
    "    return one_hot_rep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build a keras network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = [\"fonts/AGENCY.csv\"]\n",
    "X, y = import_data(data_paths)\n",
    "unique_chars = list(set(y))\n",
    "y = convert_to_one_hot(y, unique_chars)\n",
    "\n",
    "model = Sequential()\n",
    "# Layer 1: Convolution 2D layer with relu activations\n",
    "model.add(Conv2D(32, (4, 4), activation='relu', input_shape=(20, 20, 1)))\n",
    "# Layer 2: Max pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "# Layer 3: Convolution\n",
    "model.add(Conv2D(64, (4, 4), activation='relu'))\n",
    "# Layer 4: Max pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "# Layer 5: Flatten\n",
    "model.add(Flatten())\n",
    "# Layer 6: Dropout\n",
    "model.add(Dropout(0.4))\n",
    "# Layer 7: Dense with relu activation\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "# Layer 8: Dense with softmax activation\n",
    "model.add(Dense(len(unique_chars), activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Exploration and Evaluation\n",
    "\n",
    "Evaluate the network using cross validation (splitting data into training/testing). What is its accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "702/702 [==============================] - 5s 7ms/step - loss: 5.5434 - acc: 0.0043\n",
      "Epoch 2/50\n",
      "702/702 [==============================] - 1s 859us/step - loss: 5.4558 - acc: 0.0185\n",
      "Epoch 3/50\n",
      "702/702 [==============================] - 1s 801us/step - loss: 5.0458 - acc: 0.0499\n",
      "Epoch 4/50\n",
      "702/702 [==============================] - 1s 990us/step - loss: 4.1814 - acc: 0.1125\n",
      "Epoch 5/50\n",
      "702/702 [==============================] - 0s 686us/step - loss: 3.3364 - acc: 0.2251\n",
      "Epoch 6/50\n",
      "702/702 [==============================] - 1s 723us/step - loss: 2.8084 - acc: 0.2849\n",
      "Epoch 7/50\n",
      "702/702 [==============================] - 0s 656us/step - loss: 2.4305 - acc: 0.3689\n",
      "Epoch 8/50\n",
      "702/702 [==============================] - 0s 693us/step - loss: 2.1501 - acc: 0.3846\n",
      "Epoch 9/50\n",
      "702/702 [==============================] - 0s 701us/step - loss: 1.9594 - acc: 0.4231\n",
      "Epoch 10/50\n",
      "702/702 [==============================] - 0s 685us/step - loss: 1.7704 - acc: 0.4701\n",
      "Epoch 11/50\n",
      "702/702 [==============================] - 0s 674us/step - loss: 1.6306 - acc: 0.4957\n",
      "Epoch 12/50\n",
      "702/702 [==============================] - 1s 839us/step - loss: 1.5717 - acc: 0.5100\n",
      "Epoch 13/50\n",
      "702/702 [==============================] - 0s 706us/step - loss: 1.4466 - acc: 0.5442\n",
      "Epoch 14/50\n",
      "702/702 [==============================] - 1s 833us/step - loss: 1.4219 - acc: 0.5271\n",
      "Epoch 15/50\n",
      "702/702 [==============================] - 1s 826us/step - loss: 1.3390 - acc: 0.5684\n",
      "Epoch 16/50\n",
      "702/702 [==============================] - 1s 746us/step - loss: 1.2244 - acc: 0.6026\n",
      "Epoch 17/50\n",
      "702/702 [==============================] - 1s 879us/step - loss: 1.2310 - acc: 0.5869\n",
      "Epoch 18/50\n",
      "702/702 [==============================] - 1s 716us/step - loss: 1.1576 - acc: 0.6154 0s - loss: 1.1358 - acc: 0.620\n",
      "Epoch 19/50\n",
      "702/702 [==============================] - 1s 726us/step - loss: 1.1574 - acc: 0.5969\n",
      "Epoch 20/50\n",
      "702/702 [==============================] - 1s 734us/step - loss: 1.0839 - acc: 0.6239\n",
      "Epoch 21/50\n",
      "702/702 [==============================] - 1s 719us/step - loss: 1.0800 - acc: 0.6425\n",
      "Epoch 22/50\n",
      "702/702 [==============================] - 1s 718us/step - loss: 1.0250 - acc: 0.6481\n",
      "Epoch 23/50\n",
      "702/702 [==============================] - 1s 730us/step - loss: 1.0031 - acc: 0.6624\n",
      "Epoch 24/50\n",
      "702/702 [==============================] - 1s 747us/step - loss: 0.9240 - acc: 0.6738\n",
      "Epoch 25/50\n",
      "702/702 [==============================] - 1s 725us/step - loss: 0.9933 - acc: 0.6410\n",
      "Epoch 26/50\n",
      "702/702 [==============================] - 1s 726us/step - loss: 0.9093 - acc: 0.6909\n",
      "Epoch 27/50\n",
      "702/702 [==============================] - 1s 736us/step - loss: 0.8727 - acc: 0.6895\n",
      "Epoch 28/50\n",
      "702/702 [==============================] - 1s 899us/step - loss: 0.8515 - acc: 0.7051 0s - loss: 0.8290 - acc: \n",
      "Epoch 29/50\n",
      "702/702 [==============================] - 1s 747us/step - loss: 0.8736 - acc: 0.7108\n",
      "Epoch 30/50\n",
      "702/702 [==============================] - 1s 717us/step - loss: 0.8265 - acc: 0.7123\n",
      "Epoch 31/50\n",
      "702/702 [==============================] - 0s 680us/step - loss: 0.7858 - acc: 0.7379\n",
      "Epoch 32/50\n",
      "702/702 [==============================] - 1s 755us/step - loss: 0.8090 - acc: 0.7208\n",
      "Epoch 33/50\n",
      "702/702 [==============================] - 1s 812us/step - loss: 0.7771 - acc: 0.7094\n",
      "Epoch 34/50\n",
      "702/702 [==============================] - 1s 901us/step - loss: 0.7230 - acc: 0.7450\n",
      "Epoch 35/50\n",
      "702/702 [==============================] - 0s 548us/step - loss: 0.7147 - acc: 0.7365\n",
      "Epoch 36/50\n",
      "702/702 [==============================] - 0s 546us/step - loss: 0.7320 - acc: 0.7436\n",
      "Epoch 37/50\n",
      "702/702 [==============================] - 0s 566us/step - loss: 0.6834 - acc: 0.7664\n",
      "Epoch 38/50\n",
      "702/702 [==============================] - 0s 563us/step - loss: 0.6623 - acc: 0.7650\n",
      "Epoch 39/50\n",
      "702/702 [==============================] - 0s 565us/step - loss: 0.6921 - acc: 0.7450\n",
      "Epoch 40/50\n",
      "702/702 [==============================] - 0s 545us/step - loss: 0.6611 - acc: 0.7536\n",
      "Epoch 41/50\n",
      "702/702 [==============================] - 0s 544us/step - loss: 0.6742 - acc: 0.7578\n",
      "Epoch 42/50\n",
      "702/702 [==============================] - 0s 593us/step - loss: 0.6626 - acc: 0.7593\n",
      "Epoch 43/50\n",
      "702/702 [==============================] - 0s 559us/step - loss: 0.6524 - acc: 0.7707\n",
      "Epoch 44/50\n",
      "702/702 [==============================] - 0s 571us/step - loss: 0.6191 - acc: 0.7778\n",
      "Epoch 45/50\n",
      "702/702 [==============================] - 0s 544us/step - loss: 0.5940 - acc: 0.7821\n",
      "Epoch 46/50\n",
      "702/702 [==============================] - 0s 574us/step - loss: 0.5897 - acc: 0.7821\n",
      "Epoch 47/50\n",
      "702/702 [==============================] - 0s 556us/step - loss: 0.5741 - acc: 0.7920\n",
      "Epoch 48/50\n",
      "702/702 [==============================] - 0s 549us/step - loss: 0.5604 - acc: 0.7806\n",
      "Epoch 49/50\n",
      "702/702 [==============================] - 0s 595us/step - loss: 0.6028 - acc: 0.7892\n",
      "Epoch 50/50\n",
      "702/702 [==============================] - 0s 594us/step - loss: 0.5413 - acc: 0.8020\n",
      "702/702 [==============================] - 2s 2ms/step\n",
      "Train accuracy: 0.881766382276\n",
      "302/302 [==============================] - 0s 219us/step\n",
      "Test accuracy: 0.417218543244\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.3)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.fit(np.array(x_train), np.array(y_train), epochs=50, batch_size=32)  \n",
    "score = model.evaluate(np.array(x_train), np.array(y_train), batch_size=32)\n",
    "print(\"Train accuracy:\", score[1])\n",
    "score = model.evaluate(np.array(x_test), np.array(y_test), batch_size=32)\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and train a different network topology (add more convolution/dropout layers, explore other types/sizes of layer). Try to find a topology that works better than the one described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "702/702 [==============================] - 6s 9ms/step - loss: 5.6105 - acc: 0.0142\n",
      "Epoch 2/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 4.9980 - acc: 0.0869\n",
      "Epoch 3/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 3.7424 - acc: 0.2450\n",
      "Epoch 4/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 2.4814 - acc: 0.4060\n",
      "Epoch 5/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 1.7230 - acc: 0.5527\n",
      "Epoch 6/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 1.3084 - acc: 0.6068\n",
      "Epoch 7/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 1.0498 - acc: 0.6667\n",
      "Epoch 8/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 0.9152 - acc: 0.7023\n",
      "Epoch 9/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 0.8117 - acc: 0.7379\n",
      "Epoch 10/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 0.7149 - acc: 0.7593\n",
      "Epoch 11/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 0.6303 - acc: 0.7821\n",
      "Epoch 12/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 0.5653 - acc: 0.8205\n",
      "Epoch 13/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 0.5256 - acc: 0.8291\n",
      "Epoch 14/50\n",
      "702/702 [==============================] - 3s 4ms/step - loss: 0.4973 - acc: 0.8376\n",
      "Epoch 15/50\n",
      "702/702 [==============================] - 3s 4ms/step - loss: 0.4403 - acc: 0.8604\n",
      "Epoch 16/50\n",
      "702/702 [==============================] - 3s 4ms/step - loss: 0.4270 - acc: 0.8746\n",
      "Epoch 17/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 0.3916 - acc: 0.8732\n",
      "Epoch 18/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 0.3621 - acc: 0.8789\n",
      "Epoch 19/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 0.3361 - acc: 0.8875\n",
      "Epoch 20/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 0.3281 - acc: 0.8889\n",
      "Epoch 21/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 0.3196 - acc: 0.8932\n",
      "Epoch 22/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 0.3121 - acc: 0.8917\n",
      "Epoch 23/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 0.2965 - acc: 0.8889\n",
      "Epoch 24/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 0.3018 - acc: 0.8889\n",
      "Epoch 25/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 0.2744 - acc: 0.8989\n",
      "Epoch 26/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 0.2858 - acc: 0.8960\n",
      "Epoch 27/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 0.2611 - acc: 0.8974\n",
      "Epoch 28/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 0.2758 - acc: 0.9046\n",
      "Epoch 29/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 0.2478 - acc: 0.9031\n",
      "Epoch 30/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 0.2504 - acc: 0.9046\n",
      "Epoch 31/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 0.2444 - acc: 0.9088\n",
      "Epoch 32/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 0.2226 - acc: 0.9103\n",
      "Epoch 33/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 0.2136 - acc: 0.9160\n",
      "Epoch 34/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 0.2161 - acc: 0.9103\n",
      "Epoch 35/50\n",
      "702/702 [==============================] - 2s 4ms/step - loss: 0.2054 - acc: 0.9202\n",
      "Epoch 36/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 0.2195 - acc: 0.9160\n",
      "Epoch 37/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 0.2162 - acc: 0.9103\n",
      "Epoch 38/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 0.2009 - acc: 0.9231\n",
      "Epoch 39/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 0.2117 - acc: 0.9160A: 1s - loss: 0.1769 -\n",
      "Epoch 40/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 0.1985 - acc: 0.9160\n",
      "Epoch 41/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 0.1970 - acc: 0.9231\n",
      "Epoch 42/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 0.1938 - acc: 0.9259\n",
      "Epoch 43/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 0.1922 - acc: 0.9174\n",
      "Epoch 44/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 0.1895 - acc: 0.9217A: 0s - loss: 0.1689 - acc: 0\n",
      "Epoch 45/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 0.1892 - acc: 0.9145\n",
      "Epoch 46/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 0.1861 - acc: 0.9231\n",
      "Epoch 47/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 0.1742 - acc: 0.9302\n",
      "Epoch 48/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 0.1840 - acc: 0.9259A: 1s - loss: 0.1595 \n",
      "Epoch 49/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 0.1718 - acc: 0.9274\n",
      "Epoch 50/50\n",
      "702/702 [==============================] - 2s 3ms/step - loss: 0.1709 - acc: 0.9302\n",
      "702/702 [==============================] - 2s 3ms/step\n",
      "Train accuracy: 0.94301994319\n",
      "302/302 [==============================] - 0s 672us/step\n",
      "Test accuracy: 0.516556291588\n"
     ]
    }
   ],
   "source": [
    "data_paths = [\"fonts/AGENCY.csv\"]\n",
    "X, y = import_data(data_paths)\n",
    "unique_chars = list(set(y))\n",
    "y = convert_to_one_hot(y, unique_chars)\n",
    "\n",
    "model = Sequential()\n",
    "# Layer 1: Convolution 2D layer with relu activations\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(20, 20, 1)))\n",
    "# Layer 2: Max pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
    "# Layer 3: Flatten\n",
    "model.add(Flatten())\n",
    "# Layer 4: Dropout\n",
    "model.add(Dropout(0.5))\n",
    "# Layer 5: Dense with relu activation\n",
    "model.add(Dense(2048, activation='relu'))\n",
    "# Layer 6: Dense with softmax activation\n",
    "model.add(Dense(len(unique_chars), activation='softmax'))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.3)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.fit(np.array(x_train), np.array(y_train), epochs=50, batch_size=32)  \n",
    "score = model.evaluate(np.array(x_train), np.array(y_train), batch_size=32)\n",
    "print(\"Train accuracy:\", score[1])\n",
    "score = model.evaluate(np.array(x_test), np.array(y_test), batch_size=32)\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the accuracy of your network with character inputs from a DIFFERENT font set. How does it perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1004/1004 [==============================] - 8s 8ms/step - loss: 5.5535 - acc: 0.0179A: 2s - loss: 5.5915 - ac\n",
      "Epoch 2/50\n",
      "1004/1004 [==============================] - 3s 3ms/step - loss: 4.3304 - acc: 0.1434\n",
      "Epoch 3/50\n",
      "1004/1004 [==============================] - 3s 3ms/step - loss: 2.3946 - acc: 0.3904\n",
      "Epoch 4/50\n",
      "1004/1004 [==============================] - 4s 4ms/step - loss: 1.5650 - acc: 0.5458\n",
      "Epoch 5/50\n",
      "1004/1004 [==============================] - 4s 4ms/step - loss: 1.1763 - acc: 0.6424\n",
      "Epoch 6/50\n",
      "1004/1004 [==============================] - 3s 3ms/step - loss: 0.9511 - acc: 0.6932\n",
      "Epoch 7/50\n",
      "1004/1004 [==============================] - 3s 3ms/step - loss: 0.8364 - acc: 0.7351\n",
      "Epoch 8/50\n",
      " 576/1004 [================>.............] - ETA: 1s - loss: 0.6686 - acc: 0.7934"
     ]
    }
   ],
   "source": [
    "train_paths = [\"fonts/AGENCY.csv\"]\n",
    "test_paths = [\"fonts/BELL.csv\"]\n",
    "x_train, y_train = import_data(train_paths)\n",
    "x_test, y_test = import_data(test_paths)\n",
    "unique_chars = list(set(y_train).union(y_test))\n",
    "y_train = convert_to_one_hot(y_train, unique_chars)\n",
    "y_test = convert_to_one_hot(y_test, unique_chars)\n",
    "\n",
    "model = Sequential()\n",
    "# Layer 1: Convolution 2D layer with relu activations\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(20, 20, 1)))\n",
    "# Layer 2: Max pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
    "# Layer 3: Flatten\n",
    "model.add(Flatten())\n",
    "# Layer 4: Dropout\n",
    "model.add(Dropout(0.5))\n",
    "# Layer 5: Dense with relu activation\n",
    "model.add(Dense(2048, activation='relu'))\n",
    "# Layer 6: Dense with softmax activation\n",
    "model.add(Dense(len(unique_chars), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.fit(np.array(x_train), np.array(y_train), epochs=50, batch_size=32)  \n",
    "score = model.evaluate(np.array(x_train), np.array(y_train), batch_size=32)\n",
    "print(\"Train accuracy:\", score[1])\n",
    "score = model.evaluate(np.array(x_test), np.array(y_test), batch_size=32)\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your best network on inputs from the data from at least 2 different fonts. How does your accuracy compare to the 1-font case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths = [\"fonts/AGENCY.csv\", \"fonts/BELL.csv\", \"fonts/TXT.csv\"]\n",
    "test_paths = [\"fonts/AGENCY.csv\", \"fonts/BELL.csv\", \"fonts/TXT.csv\"]\n",
    "X, y = import_data(train_paths)\n",
    "unique_chars = list(set(y))\n",
    "y = convert_to_one_hot(y, unique_chars)\n",
    "\n",
    "model = Sequential()\n",
    "# Layer 1: Convolution 2D layer with relu activations\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(20, 20, 1)))\n",
    "# Layer 2: Max pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
    "# Layer 3: Flatten\n",
    "model.add(Flatten())\n",
    "# Layer 4: Dropout\n",
    "model.add(Dropout(0.5))\n",
    "# Layer 5: Dense with relu activation\n",
    "model.add(Dense(2048, activation='relu'))\n",
    "# Layer 6: Dense with softmax activation\n",
    "model.add(Dense(len(unique_chars), activation='softmax'))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.3)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.fit(np.array(x_train), np.array(y_train), epochs=50, batch_size=32)  \n",
    "score = model.evaluate(np.array(x_train), np.array(y_train), batch_size=32)\n",
    "print(\"Train accuracy:\", score[1])\n",
    "score = model.evaluate(np.array(x_test), np.array(y_test), batch_size=32)\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What accuracy do you see when testing with inputs from a font you didn't train on now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths = [\"fonts/AGENCY.csv\", \"fonts/BELL.csv\", \"fonts/TXT.csv\"]\n",
    "test_paths = [\"fonts/SNAP.csv\"]\n",
    "x_train, y_train = import_data(train_paths)\n",
    "x_test, y_test = import_data(test_paths)\n",
    "unique_chars = list(set(y_train).union(y_test))\n",
    "y_train = convert_to_one_hot(y_train, unique_chars)\n",
    "y_test = convert_to_one_hot(y_test, unique_chars)\n",
    "\n",
    "model = Sequential()\n",
    "# Layer 1: Convolution 2D layer with relu activations\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(20, 20, 1)))\n",
    "# Layer 2: Max pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
    "# Layer 3: Flatten\n",
    "model.add(Flatten())\n",
    "# Layer 4: Dropout\n",
    "model.add(Dropout(0.5))\n",
    "# Layer 5: Dense with relu activation\n",
    "model.add(Dense(2048, activation='relu'))\n",
    "# Layer 6: Dense with softmax activation\n",
    "model.add(Dense(len(unique_chars), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.fit(np.array(x_train), np.array(y_train), epochs=50, batch_size=32)  \n",
    "score = model.evaluate(np.array(x_train), np.array(y_train), batch_size=32)\n",
    "print(\"Train accuracy:\", score[1])\n",
    "score = model.evaluate(np.array(x_test), np.array(y_test), batch_size=32)\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
